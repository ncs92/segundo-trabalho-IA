{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'warnings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c3b8429a0d86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m#Pasta que contém o conjunto de dados já extraído.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#Esta pasta contém as pastas A-Z e os arquivos NIST_Train_Upper.txt, NIST_Test_Upper.txt e NIST_Valid_Upper.txt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'warnings' is not defined"
     ]
    }
   ],
   "source": [
    "import skimage.feature, skimage.io\n",
    "import numpy as np\n",
    "import math\n",
    "from math import floor\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNNC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from skimage.feature import hog\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#Pasta que contém o conjunto de dados já extraído.\n",
    "#Esta pasta contém as pastas A-Z e os arquivos NIST_Train_Upper.txt, NIST_Test_Upper.txt e NIST_Valid_Upper.txt\n",
    "NCharacter_dataset_folder = \"./exercicios/NCharacter_SD19_BMP/\"\n",
    "\n",
    "#Uma pasta onde as características extraídas são salvas.\n",
    "features_folder = 'features'\n",
    "\n",
    "\n",
    "\n",
    "#Classe que abstrai a divisão em zonas\n",
    "class Zonas(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    #Função geradora que retorna uma zona de cada vez.\n",
    "    #Os argumentos são o número de linhas da imagem e o número de colunas da imagem.\n",
    "    def get_zonas(self, n_linhas, n_colunas):\n",
    "        raise NotImplementedError\n",
    "\n",
    "#Classe que implementa a divisão em zonas retangulares.\n",
    "class ZonasRetangulares(Zonas):\n",
    "    \n",
    "    #Construtor que configura o zoneamento a ser feito: zonas_x zonas horizontais e zonas_y zonas verticais.\n",
    "    def __init__(self, zonas_x, zonas_y):\n",
    "        super(ZonasRetangulares, self).__init__()\n",
    "        self.zonas_x = zonas_x\n",
    "        self.zonas_y = zonas_y\n",
    "        \n",
    "    #Implementa a função geradora de zonas retangulares.\n",
    "    def get_zonas(self, n_linhas, n_colunas):\n",
    "        cortes_x = np.floor(np.linspace(0, n_colunas, num=self.zonas_x+1)).astype(int)\n",
    "        cortes_y = np.floor(np.linspace(0, n_linhas, num=self.zonas_y+1)).astype(int)\n",
    "        #print (cortes_x)\n",
    "        #print (cortes_y)\n",
    "        for i in range(len(cortes_x)-1):\n",
    "            for j in range(len(cortes_y)-1):\n",
    "                yield(cortes_x[i], cortes_y[j], cortes_x[i+1], cortes_y[j+1],cortes_x[i] - cortes_x[i+1], cortes_y[i] - cortes_y[i+1] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caraterísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograma_cor(imagem):\n",
    "    #print('im', imagem.shape)\n",
    "    vals, counts = np.unique(imagem, return_counts=True)\n",
    "    o = vals.argsort()\n",
    "    vals = vals[o]\n",
    "    counts = counts[o]\n",
    "    if len(vals) < 2:\n",
    "        #imagem com apenas branco ou apenas preto\n",
    "        if vals[0] == 0:\n",
    "            return [counts[0], 0]\n",
    "        else:\n",
    "            return [0, counts[0]] \n",
    "    return counts\n",
    "\n",
    "def histograma_horizontal(imagem):\n",
    "    cortes = np.linspace(0, imagem.shape[0], 4)\n",
    "    j = imagem.shape[1]\n",
    "    hh = []\n",
    "    for m in range(0, len(cortes)-1): \n",
    "        qtdPreto = 0\n",
    "        qtdBranco = 0\n",
    "        for k in range(floor(cortes[m]), floor(cortes[m+1])):            \n",
    "            for l in range(0, j):\n",
    "                if imagem[k][l] == 0:\n",
    "                    qtdPreto += 1\n",
    "                else:\n",
    "                    qtdBranco += 1\n",
    "        concatena = str(qtdBranco) + str(qtdPreto)\n",
    "        hh.append(int(concatena))\n",
    "    return hh\n",
    "        \n",
    "\n",
    "def histograma_vertical(imagem):\n",
    "    cortes = np.linspace(0, imagem.shape[1], 4)\n",
    "    j = imagem.shape[0]\n",
    "    hv = []\n",
    "    for m in range(0, len(cortes)-1): \n",
    "        qtdPreto = 0\n",
    "        qtdBranco = 0\n",
    "        for k in range(floor(cortes[m]), floor(cortes[m+1])):            \n",
    "            for l in range(0, j):\n",
    "                if imagem[l][k] == 0:\n",
    "                    qtdPreto += 1\n",
    "                else:\n",
    "                    qtdBranco += 1\n",
    "        concatena = str(qtdBranco) + str(qtdPreto)\n",
    "        hv.append(int(concatena))\n",
    "    return hv\n",
    "\n",
    "def hog_image(image):\n",
    "    return hog(image, orientations=4, pixels_per_cell=(1, 1), cells_per_block=(1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta função apenas abre os arquivos com as listas de treino / teste e validação.\n",
    "#Retorna os caminhos para os arquivos, os rótulos e o nome dos arquivos sem o caminho.\n",
    "def parse_filelist(path, prefix=''):\n",
    "    with open(path, 'r') as f:\n",
    "        c = f.readlines()\n",
    "    caminhos = list(map(str.strip, c))\n",
    "    rotulos = [ i.split('/')[1].upper() for i in caminhos]\n",
    "    arquivos = [i.split('/')[-1] for i in caminhos]\n",
    "    p = zip(rotulos, arquivos)\n",
    "    caminhos = [ prefix + '/' + i[0] + '/' + i[1] for i in p]\n",
    "    \n",
    "    return list(zip(caminhos,rotulos, arquivos))\n",
    "\n",
    "#Esta é uma função que recebe o tipo de zoneamento a ser feito e as features que devem ser\n",
    "#extraídas de cada zona. Veja que essa função é chamada no \"for\" abaixo, que faz a extração das características\n",
    "#para cada uma das listas de imagens (treino, teste e validação)\n",
    "def extract_features(filelist, dataset_folder, zonas, features=[histograma_cor, histograma_horizontal]):\n",
    "    instancias = parse_filelist(filelist, prefix=dataset_folder)\n",
    "    #Note que o MAP abaixo mapeia cada instância (imagem) à função que extrai as \n",
    "    #características (feature_extraction) abaixo.\n",
    "    features = list(map(feature_extraction, instancias, [zonas] * len(instancias), [features] * len(instancias)))\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "#Essa função de extração das características de cada instância. zonas é uma instância de subclasse\n",
    "#de Zonas. Features é uma lista de funções que extraem características. Cada função da lista recebe uma matriz\n",
    "#que representa a imagem (que está na zona) e retorna o vetor de característica computado daquela característica. \n",
    "def feature_extraction(instancia, zonas, features):\n",
    "    caminho = instancia[0]\n",
    "    #print(instancia)\n",
    "    imagem = skimage.io.imread(caminho)\n",
    "    caracteristicas = np.array([])\n",
    "    #print(\"imagem.shape\",imagem.shape)\n",
    "    \n",
    "    res = []\n",
    "    for f in features:\n",
    "        for z in zonas.get_zonas(imagem.shape[1], imagem.shape[0]):\n",
    "            #print (\"%d:%d,%d:%d\" % (z[0], z[2], z[1], z[3]))\n",
    "            f_val = f(imagem[z[0]:z[2],z[1]:z[3]])\n",
    "            res.extend(f_val)\n",
    "    \n",
    "    return np.array(res)\n",
    "\n",
    "#Realiza a extração das características!\n",
    "for i in [('NIST_Train_Upper.txt', 'train'), ('NIST_Test_Upper.txt', 'test'), ('NIST_Valid_Upper.txt', 'val')]:\n",
    "    print('extraindo características de %s' % (i[0]))\n",
    "    #note que esta linha extrai características de 4 zonas (2 imagens por linhas e 2 por coluna). \n",
    "    #Neste exemplo apenas a característica histograma_cor é computada.\n",
    "    feats = extract_features(NCharacter_dataset_folder + i[0], NCharacter_dataset_folder, ZonasRetangulares(2,2), features=[histograma_cor, histograma_horizontal])\n",
    "    \n",
    "    #Extraia aqui outras características! Escolha outros zoneamentos! (Só não esqueça de concatenar tudo em feats)\n",
    "    #Estude a função np.concatenate do numpy para concatenar!\n",
    "    \n",
    "    #np.save(open(features_folder + ('/%s_feats.pkl' % (i[1])), 'wb'), feats )\n",
    "    np.save(features_folder + ('/%s_feats.pkl' % (i[1])), feats)\n",
    "\n",
    "print('Fim da extração de características!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino e Teste com SVM e KNN (Sem validação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.load(features_folder + '/train_feats.pkl.npy')\n",
    "train_rotulos = parse_filelist(NCharacter_dataset_folder + 'NIST_Train_Upper.txt')\n",
    "train_rotulos = [i[1] for i in train_rotulos]\n",
    "#print (len(train_rotulos), train_features.shape)\n",
    "\n",
    "test_features = np.load(features_folder + '/test_feats.pkl.npy')\n",
    "test_rotulos = parse_filelist(NCharacter_dataset_folder + 'NIST_Test_Upper.txt')\n",
    "test_rotulos = [i[1] for i in test_rotulos]\n",
    "print (len(test_rotulos), test_features.shape)\n",
    "\n",
    "SS = StandardScaler()\n",
    "SS.fit(train_features)\n",
    "train_features = SS.transform(train_features)\n",
    "test_features = SS.transform(test_features)\n",
    "\n",
    "#KKKKKKKKKKKKKNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
    "KNN = KNNC(n_neighbors=3)\n",
    "\n",
    "KNN.fit(train_features, train_rotulos)\n",
    "\n",
    "\n",
    "y_pred = KNN.predict(test_features)\n",
    "\n",
    "print('KNN score: ' , '{:.3f}'.format(accuracy_score(test_rotulos, y_pred)))\n",
    "#print(confusion_matrix(test_rotulos, y_pred))\n",
    "\n",
    "print(classification_report(test_rotulos, y_pred, digits=3))\n",
    "\n",
    "#SSSSSSSSSSSVVVVVVVVVVVMMMMMMMMMMMMMMMM\n",
    "clf = SVC()\n",
    "clf.fit(train_features, train_rotulos)\n",
    "y_pred = clf.predict(test_features)\n",
    "\n",
    "print('SVM score: ', '{:.3f}'.format(accuracy_score(test_rotulos, y_pred)))\n",
    "#print(confusion_matrix(test_rotulos, y_pred))\n",
    "\n",
    "print(classification_report(test_rotulos, y_pred, digits=3))\n",
    "\n",
    "\n",
    "#ARVORE DECISÃO\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train_features, train_rotulos)\n",
    "clf.predict(test_features)\n",
    "clf.predict_proba(test_features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
